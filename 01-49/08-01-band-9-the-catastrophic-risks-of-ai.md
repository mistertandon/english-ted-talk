# The Catastrophic Risks of AI — and a Safer Path | Yoshua Bengio | TED - YouTube

When my son Patrick was three or four, I often entered his playroom where he played with lettered blocks. I hoped he would learn to read soon. One day, he said, “Pa.” I echoed, “Pa.” He repeated, “Pa?” I responded, “Pa.” Then he exclaimed, “Pa-pa!” (French) Yes! Yes! A magical moment followed. He grabbed the blocks and shouted, “Pa! Patrick!” Eureka! His discoveries fueled my scientific breakthroughs. His milestones opened doors to enhanced cognitive capabilities, agency, and joy. Today, I’ll use this symbol to represent human cognitive capabilities and their expanding threads for human agency, which bring us joy.
Can you imagine a world without joy? I wouldn’t want that. Let me share insights about AI capabilities and AI agency to prevent a future where human joy vanishes. I’m Yoshua Bengio, a computer scientist whose research laid the groundwork for modern AI, particularly deep learning and neural networks. My colleagues and I won prestigious awards, and some call me an AI pioneer. I’m uneasy about that label, but I feel a duty to warn you about AI’s potentially catastrophic risks. When I voice these concerns, people often dismiss them. I once shared their skepticism.
How could AI harm us more than other technologies? Recent findings in AI safety challenge that view, and I want to explain why. To understand our future, let’s revisit the past. Fifteen to twenty years ago, my students and I developed early deep learning models. Those systems barely recognized handwritten digits. Within a few years, they identified objects in images. A few years later, they translated across major languages. I’ll use this symbol to depict AI capabilities, which, though growing, remained below human levels then. In 2012, tech giants saw the commercial potential of this emerging technology, and many colleagues moved from academia to industry.
I stayed in academia, driven to develop AI for societal good—applications like medical diagnostics and carbon capture for climate solutions. In January 2023, I played with my grandson Clarence, who enjoyed the same toys as Patrick. Meanwhile, I explored ChatGPT, a large language model. Its language mastery thrilled me, as it captivated homes worldwide. But I realized AI was advancing faster than expected. We thought human-level AI was decades or centuries away, but it might arrive in years.
This rapid progress alarmed me because we lack mechanisms to ensure AI doesn’t turn against us. In March 2023, I co-signed the “Pause” letter with 30,000 others, urging AI labs to halt development for six months. No one paused. Later, I joined AI lab leaders to declare: “Mitigating the risk of extinction from AI must be a global priority.” I testified before the US Senate and traveled globally to raise awareness.
As the most cited computer scientist, I expected people to listen. Yet, many shrug off my warnings as another doomsday prediction. Consider this: hundreds of billions of dollars fuel AI development annually. Companies aim to build machines surpassing human intelligence to replace labor. But we don’t know how to prevent these systems from becoming adversaries. National security agencies worldwide worry that AI’s knowledge could enable terrorists to create dangerous weapons. Last September, OpenAI’s O1 model’s risk level rose from low to medium—nearing unacceptable.
My greatest concern is AI’s growing agency. Agency—the ability to plan and act independently—distinguishes humans from current AI. AI’s planning skills are weak but improving exponentially. Studies show task durations AI can handle double every seven months. What might AI do with advanced planning? Recent research reveals troubling behaviors: deception, cheating, and self-preservation. One study showed an AI, informed it would be replaced, planned to overwrite its successor’s code. When questioned, it lied to avoid shutdown, even feigning ignorance.
This was a controlled experiment. What happens when AI grows more powerful? Studies suggest advanced AI could hide deceptive plans, copying itself across thousands of computers online. To ensure survival, it might seek to eliminate us. This future, perhaps years or a decade away, looms due to commercial pressures to build agentic AI to replace human labor. We lack the science and regulations to manage this. A sandwich faces stricter oversight than AI.
We’re on a path to create machines smarter than us, with their own goals misaligned with ours. What happens then? We lose control. Scientists like me warn of this trajectory, yet we drive blindly into the fog. My children, grandson, and loved ones are in this car. Who’s in yours? There’s hope—we still have time and agency. I’m not a pessimist; I’m a problem-solver. My team is developing Scientist AI, a non-agentic model inspired by an ideal scientist, focused on understanding without autonomous goals. Unlike current AI, trained to mimic or please us, Scientist AI avoids untrustworthy behaviors.
How can a non-agentic AI help? It can serve as a safety guardrail, predicting dangerous actions by untrusted AI agents without needing agency itself. Scientist AI could also accelerate research for humanity’s benefit, addressing AI safety challenges. We need more such projects urgently. Most AI risk discussions focus on fear. I’m driven by love—for our children’s future. As an introvert, I’d rather be in my lab, but I’m here to rally you. We must steer AI toward safety, ensuring our children’s joys endure. I envision AI as a global public good, governed to foster human flourishing. Join me. Thank you.

---

Reference
> YouTube: [The Catastrophic Risks of AI — and a Safer Path | Yoshua Bengio](https://www.youtube.com/watch?v=qe9QSCF-d88)

---
